# Real-Time E-commerce Analytics Platform

## âœ¨ Project Overview

This project is a **Real-Time E-commerce Analytics Platform** that captures, processes, and visualizes e-commerce events as they occur. It is designed with a modern, scalable data streaming architecture using **Apache Kafka**, **Apache Spark**, **PostgreSQL**, and **Streamlit** to provide instantaneous insights into user behavior, product performance, and sales trends.

### ğŸ”— Key Features

* **â± Real-Time Data Ingestion**: Seamlessly capture e-commerce events like page views and purchases.
* **ğŸŒ€ Stream Processing**: Leverage Spark Streaming to perform real-time aggregations and metric calculations.
* **ğŸ“‚ Persistent Storage**: Store processed data in PostgreSQL for historical analysis.
* **ğŸ“Š Interactive Dashboards**: Visualize real-time and historical analytics using Streamlit.
* **ğŸš€ Scalable Architecture**: Built to handle high-throughput event streams and scalable processing workloads.

---

## âš™ï¸ Architecture Overview

The system is composed of modular services, each responsible for a different part of the data pipeline:

### 1. ğŸ¦ Event Generation (Producers)

* `event_generator.py`: Uses `Faker` to simulate realistic e-commerce user activity.
* `kafka_producer.py`: Publishes the generated events to Kafka topics.

### 2. ğŸ“Š Event Streaming (Kafka & Zookeeper)

* **Kafka**: Handles real-time message streaming between producers and consumers.
* **Zookeeper**: Coordinates distributed Kafka brokers.
* Events are published to topics like `ecommerce-page-views` and `ecommerce-purchases`.

### 3. ğŸš€ Stream Processing (Apache Spark)

* `spark_streaming_job.py`: Consumes events from Kafka and processes them using Spark Structured Streaming.
* `aggregation_engine.py`: Updates PostgreSQL with aggregated metrics:

  * `product_performance`
  * `user_activity`
  * `hourly_sales`

### 4. ğŸ“‚ Data Storage (PostgreSQL)

* PostgreSQL holds aggregated, structured data.
* Tables are initialized via `sql/init_tables.sql`.

### 5. ğŸ“ˆ Data Visualization (Streamlit)

* `dashboard/Home.py`: Main entry point for the interactive dashboard.
* Dashboard pages include:

  * Hourly Sales
  * Product Performance
  * Real-Time Metrics
  * User Activity

---

## ğŸ› ï¸ Technologies Used

| Technology    | Role                               |
| ------------- | ---------------------------------- |
| Kafka         | Real-time event streaming          |
| Zookeeper     | Kafka coordination                 |
| Spark         | Stream processing engine           |
| PostgreSQL    | Relational storage                 |
| Python        | Scripting and business logic       |
| Streamlit     | Dashboard and visualization        |
| Faker         | Synthetic data generation          |
| Docker        | Containerization and orchestration |
| Plotly/Altair | Visual analytics in Streamlit      |
| `dotenv`      | Environment variable management    |

---

## ğŸ“… Setup & Installation

### âœ… Prerequisites

* **Docker Desktop** (Engine + Compose): [Install Docker](https://www.docker.com/products/docker-desktop)
* **Python 3.8+**: [Install Python](https://www.python.org/downloads/)

### â‘  Clone the Repository

```bash
git clone (https://github.com/VamshiSunnam/real-time-ecommerce-analytics.git)
cd real-time-ecommerce-analytics
```

### â‘¡ Configure Environment Variables

Create a `.env` file in the project root:

```dotenv
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=ecommerce_analytics
POSTGRES_USER=admin
POSTGRES_PASSWORD=password123
```

### â‘¢ Start Docker Services

Run the entire stack (Kafka, Zookeeper, PostgreSQL, Spark):

```bash
docker-compose up -d
```

Verify container status:

```bash
docker-compose ps
```

### â‘£ Install Python Dependencies

```bash
python -m venv venv
# Windows:
.\venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

pip install -r requirements.txt
```

### â‘¤ Start Event Producers

Simulate event generation:

```bash
python run_streaming.py
```

### â‘¥ Start Spark Stream Processor

```bash
python consumers/spark_streaming_job.py
```

### â‘¦ Launch Streamlit Dashboard

```bash
streamlit run dashboard/Home.py
```

Navigate to: [http://localhost:8501](http://localhost:8501)

---

## ğŸ“„ Project Structure

```
real-time-ecommerce-analytics/
â”œâ”€â”€ .env                     # Environment variables
â”œâ”€â”€ config.py                # Global config
â”œâ”€â”€ docker-compose.yml       # Docker services setup
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ run_streaming.py         # Event orchestration
â”œâ”€â”€ producers/               # Event generators
â”‚   â”œâ”€â”€ event_generator.py
â”‚   â””â”€â”€ kafka_producer.py
â”œâ”€â”€ consumers/               # Spark jobs
â”‚   â”œâ”€â”€ spark_streaming_job.py
â”‚   â””â”€â”€ aggregation_engine.py
â”œâ”€â”€ dashboard/               # Streamlit app
â”‚   â”œâ”€â”€ Home.py
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ Hourly_Sales.py
â”‚       â”œâ”€â”€ Product_Performance.py
â”‚       â”œâ”€â”€ Real_Time_Metrics.py
â”‚       â””â”€â”€ User_Activity.py
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init_tables.sql      # PostgreSQL schema
â””â”€â”€ data/
    â””â”€â”€ sample_products.json
```

---

## ğŸ” Dashboard Insights

* **Hourly Sales:** Revenue, order count, unique customers over time.
* **Product Performance:** Views, purchases, conversion rates.
* **Real-Time Metrics:** Key indicators such as revenue, AOV, session count.
* **User Activity:** Engagement trends and user interactions.

---

## ğŸ§‘â€ğŸ’» Future Enhancements

* [ ] Add support for more event types (e.g. cart updates, search)
* [ ] Implement user sessionization & funnel analysis
* [ ] Integrate machine learning models for prediction & recommendation
* [ ] Deploy with user authentication for dashboards
* [ ] Enable cloud deployment on AWS/GCP
* [ ] Setup monitoring with Prometheus + Grafana
* [ ] Implement alerting system for critical metrics

---

## ğŸš€ Contributing

We welcome contributions! Feel free to submit issues or pull requests.

---

## âœ‰ï¸ Contact

For questions, feel free to reach out or open an issue.

---

## Â© License

MIT License. See `LICENSE` file for details.
